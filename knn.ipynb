{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors: Predicting King County Housing Prices\n",
    "\n",
    "<img src=\"neighbors-talking-over-fence-min.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you predict the price of a house that is about to go on sale?\n",
    "\n",
    "<img src='For-sale-sign.jpg' alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar houses should be similar in price\n",
    "\n",
    "* Square footage\n",
    "* Number of floors\n",
    "* Location\n",
    "\n",
    "\n",
    "## Distance as a measure of similarity\n",
    "\n",
    "How 'far away' are houses from each other given all of their features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is K-Nearest Neighbors?\n",
    "\n",
    "**_K-Nearest Neighbors_** (or KNN, for short) is a supervised learning algorithm that can be used for both **_Classification_** and **_Regression_** tasks. KNN is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between 2 points, the more similar they are. In KNN, each column acts as a dimension. In a dataset with two columns, we can easily visualize this by treating values for one column as X coordinates and and the other as Y coordinates. Since this is a **_Supervised Learning Algorithm_**, we must also have the labels for each point in our dataset, or else we can't use this algorithm for prediction.\n",
    "\n",
    "## Fitting the Model\n",
    "\n",
    "KNN is unique compared to other algorithms in that it does almost nothing during the \"fit\" step, and all the work during the \"predict\" step. During the 'fit' step, KNN just stores all the training data and corresponding values. No distances are calculated at this point. \n",
    "\n",
    "## Making Predictions with K\n",
    "\n",
    "All the magic happens during the 'predict' step. During this step, KNN takes a point that we want a class prediction for, and calculates the distances between that point and every single point in the training set. It then finds the `K` closest points, or **_Neighbors_**, and examines the values of each. You can think of each of the K-closest points getting a 'vote' about the predicted value. Often times the mean of all the values is taken to make a prediction about the new point.\n",
    "\n",
    "In the following animation, K=3.\n",
    "\n",
    "<img src='knn.gif'>\n",
    "\n",
    "## Distance Metrics\n",
    "\n",
    "As we explored in a previous lesson, there are different **_distance metrics_** when using KNN. For KNN, we can use **_Manhattan_**, **_Euclidean_**, or **_Minkowski Distance_**--from an algorithmic standpoint, it doesn't matter which! However, it should be noted that from a practical standpoint, these can affect our results and our overall model performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import euclidean as euc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from visualize import generate_moons_df, preprocess, plot_boundaries\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare KNN Classifier and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = preprocess(generate_moons_df(n_samples= 20, noise=0.1))\n",
    "\n",
    "# fit knn model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled.drop('target', axis=1), y_train)\n",
    "\n",
    "# fit logistic\n",
    "logistic = LogisticRegression(solver='liblinear')\n",
    "logistic.fit(X_train_scaled.drop('target', axis=1), y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "axes[0].set_title(f'KNeighborsClassifier with k={knn.n_neighbors}')\n",
    "plot_boundaries(knn, X_test_scaled, X_train_scaled, ax=axes[0], show_test=True, plot_probas=False)\n",
    "\n",
    "axes[1].set_title('Logistic Regression')\n",
    "plot_boundaries(logistic, X_test_scaled, X_train_scaled, ax=axes[1], plot_probas=False, show_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here logistic regression without any feature engineering can only give us a linear boundary but KNN can begin to follow the non-linear relationships in our data already.\n",
    "\n",
    "The dots outlined in black represent data points from our test set. Looking at these points can we calculate our accuracy? False positives, False Negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Transforming\n",
    "\n",
    "Sklearn is one of the most popular ML libraries for python which gives us access to a wealth of different algorthims. All of these algorthims follow the same API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = model_object()\n",
    "\n",
    "model.fit()\n",
    "\n",
    "model.predict()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our own implementation of KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = pd.read_csv('data/kc-house-data.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit our predictions to the middle 80% of our dataset\n",
    "\n",
    "It is easier to make predictions where the data is most dense but doing this means that any predictions made outside of the range of values we are training on will be highly suspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10 = np.percentile(house_data['price'], 10)\n",
    "top_10 = np.percentile(house_data['price'], 90)\n",
    "\n",
    "house_data = house_data[(house_data['price'] > bottom_10) & (house_data['price'] < top_10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(house_data['price'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sqft_living', 'lat', 'long']\n",
    "\n",
    "X = house_data[features]\n",
    "y = house_data['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to scale our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_knn = KNN()\n",
    "my_knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is this so slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will run for a long time\n",
    "preds = my_knn.predict(X_test_scaled, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use Sklearn's KNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
    "\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "sk_preds = nn.predict(X_test_scaled)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, sk_preds))\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the optimal number of neighbors: Model behavior with increasing k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are visualizations of a classification problem\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 10), sharey=True, sharex=True)\n",
    "\n",
    "X_train_scaled_fake, X_test_scaled_fake, y_train_fake, y_test_fake = preprocess(generate_moons_df(n_samples=100, noise=0.1))\n",
    "\n",
    "ks = [1, 2, 5, 10 , 15, 30]\n",
    "\n",
    "for k, ax in zip(ks, axes.flatten()):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled_fake.drop('target', axis=1), y_train_fake)\n",
    "    \n",
    "    train_preds = knn.score(X_train_scaled_fake.drop('target', axis=1), y_train_fake)\n",
    "    test_preds = knn.score(X_test_scaled_fake.drop('target', axis=1), y_test_fake)\n",
    "    \n",
    "    ax.set_title(f'k={knn.n_neighbors} \\n train acc {train_preds:.2f} \\n test acc: {test_preds:.2f}')\n",
    "    plot_boundaries(knn, X_test_scaled_fake, X_train_scaled_fake, ax=ax)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we say about the bias and variance of these models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal k for King County Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 30)\n",
    "\n",
    "test_errors = np.zeros(len(list(ks)))\n",
    "\n",
    "for i, k in enumerate(ks):\n",
    "    \n",
    "    nn = KNeighborsRegressor(n_neighbors=k, n_jobs=-1)\n",
    "\n",
    "    nn.fit(X_train_scaled, y_train)\n",
    "    test_preds = nn.predict(X_test_scaled)\n",
    "    \n",
    "    test_errors[i] = np.sqrt(mean_squared_error(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(list(ks), test_errors)\n",
    "ax.axvline(list(ks)[np.argmin(test_errors)], linestyle='--', color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = list(ks)[np.argmin(test_errors)]\n",
    "\n",
    "optimal_error = np.min(test_errors)\n",
    "\n",
    "print(f'Optimal number of Neighbors: {optimal_k} Root Mean Squared Error: {optimal_error:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
